{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e1f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from run_ib import RunIB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stirling import stirling\n",
    "import enumerate_lexicons\n",
    "pd.options.display.max_rows = 250\n",
    "import scipy\n",
    "from helper_functions import *\n",
    "from run_ib_new import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58368b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: _ib (this includes custom initialization)\n",
    "DEFAULT_NUM_ITER=10\n",
    "PRECISION = 1e-16\n",
    "def _ib(p_x, p_y_x, Z, gamma, init, num_iter=DEFAULT_NUM_ITER, temperature = 1):\n",
    "    \"\"\" Find encoder q(Z|X) to minimize J = I[X:Z] - gamma * I[Y:Z].\n",
    "    \n",
    "    Input:\n",
    "    p_x : Distribution on X, of shape X.\n",
    "    p_y_x : Conditional distribution on Y given X, of shape X x Y.\n",
    "    gamma : A non-negative scalar value.\n",
    "    Z : Support size of Z.\n",
    "\n",
    "    Output: \n",
    "    Conditional distribution on Z given X, of shape X x Z.\n",
    "\n",
    "    \"\"\"\n",
    "    # Support size of X\n",
    "    X = p_x.shape[-1]\n",
    "\n",
    "    # Support size of Y\n",
    "    Y = p_y_x.shape[-1]\n",
    "\n",
    "    # Randomly initialize the conditional distribution q(z|x)\n",
    "    q_z_x = init #scipy.special.softmax(np.random.randn(X, Z), -1) # shape X x Z\n",
    "    p_y_x = p_y_x[:, None, :] # shape X x 1 x Y\n",
    "    p_x = p_x[:, None] # shape X x 1\n",
    "\n",
    "    # Blahut-Arimoto iteration to find the minimizing q(z|x)\n",
    "    for _ in range(num_iter):\n",
    "        q_xz = p_x * q_z_x # Joint distribution q(x,z), shape X x Z\n",
    "        q_z = q_xz.sum(axis=0, keepdims=True) # Marginal distribution q(z), shape 1 x Z\n",
    "        q_y_z = ((q_xz / q_z)[:, :, None] * p_y_x).sum(axis=0, keepdims=True) # Conditional decoder distribution q(y|z), shape 1 x Z x Y\n",
    "        d = ( \n",
    "            scipy.special.xlogy(p_y_x, p_y_x)\n",
    "            - scipy.special.xlogy(p_y_x, q_y_z) # negative KL divergence -D[p(y|x) || q(y|z)]\n",
    "        ).sum(axis=-1) # expected distortion over Y; shape X x Z\n",
    "        q_z_x = scipy.special.softmax((np.log(q_z) - gamma*d)/temperature, axis=-1) # Conditional encoder distribution q(z|x) = 1/Z q(z) e^{-gamma*d}\n",
    "\n",
    "    return q_z_x\n",
    "\n",
    "num_dists = 3\n",
    "pgs_dists = [0,0.789,-1.315]\n",
    "num_words = 9\n",
    "num_meanings = num_dists * 3\n",
    "\n",
    "# function to make the non-deterministic frontier\n",
    "def make_curve(mu, logsp=np.logspace(2, 0, num=1500), pgs=pgs_dists):\n",
    "    init = np.identity(num_words)\n",
    "\n",
    "    qW_M = []\n",
    "    informativity = []\n",
    "    complexity = []\n",
    "\n",
    "    for gamma in logsp:\n",
    "        # x = RunIB(mu, gamma, num_dists, pgs_dists)\n",
    "        # shape: M x U x W\n",
    "        x = RunIB(mu, num_dists, pgs_dists)\n",
    "        p_m = x.prior\n",
    "        p_u_m = x.prob_u_given_m\n",
    "        p_um = p_m[:, None] * p_u_m\n",
    "        p_u = p_um.sum(axis=-1, keepdims=True)\n",
    "        q_w_m = _ib(p_m, p_u_m, num_words, gamma, init, num_iter = 20)\n",
    "        informativity_temp, complexity_temp = information_plane(p_m, p_u_m, q_w_m)\n",
    "\n",
    "        qW_M.append(q_w_m)\n",
    "        informativity.append(informativity_temp)\n",
    "        complexity.append(complexity_temp)\n",
    "        init = q_w_m\n",
    "        \n",
    "    curve = pd.DataFrame(data = {'gamma': logsp,\n",
    "                    'informativity' : informativity,\n",
    "                    'complexity' : complexity,\n",
    "                    'J' : complexity - logsp*informativity})\n",
    "    return curve, qW_M\n",
    "\n",
    "# function to find the objective function\n",
    "def get_objective(p_m, p_u_m, q_w_m, gamma):\n",
    "    informativity, complexity = information_plane(p_m, p_u_m, q_w_m)\n",
    "    return complexity - gamma * informativity\n",
    "\n",
    "# function to find gamma minimizing the objective function\n",
    "def find_gamma_index(p_m, p_u_m, q_w_m, curve):\n",
    "    objs = np.array([get_objective(p_m, p_u_m, q_w_m, gamma) for gamma in logsp])\n",
    "    objs_2 = curve[\"J\"].values\n",
    "    diff = objs - objs_2\n",
    "    return(diff.argmin())\n",
    "\n",
    "# function to calculate efficiency loss (Zaslavsky et al., 2018)\n",
    "def find_epsilon(p_m, p_u_m, q_w_m, curve):\n",
    "    objs = np.array([get_objective(p_m, p_u_m, q_w_m, gamma) for gamma in logsp])\n",
    "    objs_2 = curve[\"J\"].values\n",
    "    diff = objs - objs_2\n",
    "    return(diff.min()/logsp[diff.argmin()])\n",
    "\n",
    "# codes from Zaslavsky et al. (2018)\n",
    "def xlogx(v):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        return np.where(v > PRECISION, v * np.log2(v), 0)\n",
    "    \n",
    "def H(p, axis=None):\n",
    "    \"\"\" Entropy \"\"\"\n",
    "    return -xlogx(p).sum(axis=axis)\n",
    "\n",
    "def MI(pXY):\n",
    "    \"\"\" mutual information, I(X;Y) \"\"\"\n",
    "    return H(pXY.sum(axis=0)) + H(pXY.sum(axis=1)) - H(pXY)\n",
    "\n",
    "# function to calculate gNID (Zaslavsky et al., 2018)\n",
    "def gNID(pW_X, pV_X, pX):\n",
    "    if len(pX.shape) == 1:\n",
    "        pX = pX[:, None]\n",
    "    elif pX.shape[0] == 1 and pX.shape[1] > 1:\n",
    "        pX = pX.T\n",
    "    pXW = pW_X * pX\n",
    "    pWV = pXW.T.dot(pV_X)\n",
    "    pWW = pXW.T.dot(pW_X)\n",
    "    pVV = (pV_X * pX).T.dot(pV_X)\n",
    "    score = 1 - MI(pWV) / (np.max([MI(pWW), MI(pVV)]))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e703afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part generates the non-deterministic optimal frontier, with gamma ranging from 1 to 1000 (it takes ~2h)\n",
    "logsp = np.logspace(3,0,num = 1500)\n",
    "mu = 0.2\n",
    "curve, qW_M = make_curve(mu, logsp, pgs_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ccaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part generates all possible lexicons\n",
    "x = RunIB(mu, num_dists, pgs_dists)\n",
    "num_meanings = 3 * num_dists\n",
    "lexicon_size_range = range(2, num_meanings + 1)\n",
    "sim_lex_dict = {lexicon_size: [lexicon for lexicon in enumerate_lexicons.enumerate_possible_lexicons(num_meanings, lexicon_size)] for \n",
    "        lexicon_size in lexicon_size_range}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36544621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cshnican/opt/anaconda3/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/cshnican/opt/anaconda3/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/cshnican/opt/anaconda3/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "/Users/cshnican/opt/anaconda3/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Data frame containing real lexicons\n",
    "lexicons = x.get_real_langs(num_meanings)\n",
    "\n",
    "df = pd.DataFrame([{dm: l[1].argmax(axis=1)[dm_num]\n",
    "                        for dm_num, dm in enumerate(x.deictic_index)} for l in lexicons])\n",
    "information_plane_list = [information_plane(x.prior, x.prob_u_given_m, l[1]) for l in lexicons]\n",
    "df[\"I[U;W]\"] = [l[0] for l in information_plane_list]\n",
    "df[\"I[M;W]\"] = [l[1] for l in information_plane_list]\n",
    "df[\"gamma_fit\"] = [logsp[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)] for l in lexicons]\n",
    "df[\"epsilon\"] = [find_epsilon(x.prior, x.prob_u_given_m, l[1], curve) for l in lexicons]\n",
    "df[\"gNID\"] = [gNID(l[1], qW_M[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)], x.prior) for l in lexicons]\n",
    "df[\"Language\"] = [l[0] for l in lexicons]\n",
    "df[\"Area\"] = [l[2] for l in lexicons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae786d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame containing simulated lexicons\n",
    "lexicons_sim = []\n",
    "for lexicon_size in range(2, num_meanings+1):\n",
    "    all_lex = sim_lex_dict[lexicon_size]\n",
    "    lexicons_sim += [(\"simulated\", l[1], \"simulated\") for l in all_lex]\n",
    "\n",
    "df_sim = pd.DataFrame([{dm: l[1].argmax(axis=1)[dm_num]\n",
    "                        for dm_num, dm in enumerate(x.deictic_index)} for l in lexicons_sim])\n",
    "\n",
    "information_plane_list_sim = [information_plane(x.prior, x.prob_u_given_m, l[1]) for l in lexicons_sim]\n",
    "df_sim[\"I[U;W]\"] = [l[0] for l in information_plane_list_sim]\n",
    "df_sim[\"I[M;W]\"] = [l[1] for l in information_plane_list_sim]\n",
    "df_sim[\"gamma_fit\"] = [logsp[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)] for l in lexicons_sim]\n",
    "df_sim[\"epsilon\"] = [find_epsilon(x.prior, x.prob_u_given_m, l[1], curve) for l in lexicons_sim]\n",
    "df_sim[\"gNID\"] = [gNID(l[1], qW_M[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)], x.prior) for l in lexicons_sim]\n",
    "df_sim[\"Language\"] = [l[0] for l in lexicons_sim]\n",
    "df_sim[\"Area\"] = [l[2] for l in lexicons_sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe0a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  1  /  220 ; lang =  \tKabiy√© (Atlantic-Congo, Gur)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/helper_functions.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Language'][i] = df['Language'][i] + str(k)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  1  /  21146 ; lang =  simulated152\n",
      "i =  501  /  21146 ; lang =  simulated3002\n",
      "i =  1001  /  21146 ; lang =  simulated1637\n",
      "i =  1501  /  21146 ; lang =  simulated565\n",
      "i =  2001  /  21146 ; lang =  simulated1792\n",
      "i =  2501  /  21146 ; lang =  simulated266\n",
      "i =  3001  /  21146 ; lang =  simulated809\n",
      "i =  3501  /  21146 ; lang =  simulated14302\n",
      "i =  4001  /  21146 ; lang =  simulated6929\n",
      "i =  4501  /  21146 ; lang =  simulated1562\n",
      "i =  5001  /  21146 ; lang =  simulated6070\n",
      "i =  5501  /  21146 ; lang =  simulated5189\n",
      "i =  6001  /  21146 ; lang =  simulated6499\n",
      "i =  6501  /  21146 ; lang =  simulated6160\n",
      "i =  7001  /  21146 ; lang =  simulated12277\n",
      "i =  7501  /  21146 ; lang =  simulated1832\n",
      "i =  8001  /  21146 ; lang =  simulated14741\n",
      "i =  8501  /  21146 ; lang =  simulated7838\n",
      "i =  9001  /  21146 ; lang =  simulated12560\n",
      "i =  9501  /  21146 ; lang =  simulated13475\n",
      "i =  10001  /  21146 ; lang =  simulated7791\n",
      "i =  10501  /  21146 ; lang =  simulated10573\n",
      "i =  11001  /  21146 ; lang =  simulated20419\n",
      "i =  11501  /  21146 ; lang =  simulated14289\n",
      "i =  12001  /  21146 ; lang =  simulated14538\n",
      "i =  12501  /  21146 ; lang =  simulated19625\n",
      "i =  13001  /  21146 ; lang =  simulated11365\n",
      "i =  13501  /  21146 ; lang =  simulated19238\n",
      "i =  14001  /  21146 ; lang =  simulated13699\n",
      "i =  14501  /  21146 ; lang =  simulated6104\n",
      "i =  15001  /  21146 ; lang =  simulated6095\n",
      "i =  15501  /  21146 ; lang =  simulated14412\n",
      "i =  16001  /  21146 ; lang =  simulated13143\n",
      "i =  16501  /  21146 ; lang =  simulated13258\n",
      "i =  17001  /  21146 ; lang =  simulated20420\n",
      "i =  17501  /  21146 ; lang =  simulated16566\n",
      "i =  18001  /  21146 ; lang =  simulated17676\n",
      "i =  18501  /  21146 ; lang =  simulated14819\n",
      "i =  19001  /  21146 ; lang =  simulated20380\n",
      "i =  19501  /  21146 ; lang =  simulated11848\n",
      "i =  20001  /  21146 ; lang =  simulated17466\n",
      "i =  20501  /  21146 ; lang =  simulated12264\n",
      "i =  21001  /  21146 ; lang =  simulated19531\n"
     ]
    }
   ],
   "source": [
    "df = consistency(df)\n",
    "df_sim = consistency(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f892ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save things\n",
    "df.to_csv('sheets/real_lexicons_fit_mu_' + str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) + '.csv')\n",
    "df_sim.to_csv('sheets/sim_lexicons_fit_mu_'+ str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) +'.csv')\n",
    "curve.to_csv('sheets/ib_curve_non_deter_mu_' + str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058f5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
