{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e1f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from run_ib import RunIB\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stirling import stirling\n",
    "import enumerate_lexicons\n",
    "pd.options.display.max_rows = 250\n",
    "import scipy\n",
    "from helper_functions import *\n",
    "from run_ib_new import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58368b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: _ib (this includes custom initialization)\n",
    "DEFAULT_NUM_ITER=10\n",
    "PRECISION = 1e-16\n",
    "def _ib(p_x, p_y_x, Z, gamma, init, num_iter=DEFAULT_NUM_ITER, temperature = 1):\n",
    "    \"\"\" Find encoder q(Z|X) to minimize J = I[X:Z] - gamma * I[Y:Z].\n",
    "    \n",
    "    Input:\n",
    "    p_x : Distribution on X, of shape X.\n",
    "    p_y_x : Conditional distribution on Y given X, of shape X x Y.\n",
    "    gamma : A non-negative scalar value.\n",
    "    Z : Support size of Z.\n",
    "\n",
    "    Output: \n",
    "    Conditional distribution on Z given X, of shape X x Z.\n",
    "\n",
    "    \"\"\"\n",
    "    # Support size of X\n",
    "    X = p_x.shape[-1]\n",
    "\n",
    "    # Support size of Y\n",
    "    Y = p_y_x.shape[-1]\n",
    "\n",
    "    # Randomly initialize the conditional distribution q(z|x)\n",
    "    q_z_x = init #scipy.special.softmax(np.random.randn(X, Z), -1) # shape X x Z\n",
    "    p_y_x = p_y_x[:, None, :] # shape X x 1 x Y\n",
    "    p_x = p_x[:, None] # shape X x 1\n",
    "\n",
    "    # Blahut-Arimoto iteration to find the minimizing q(z|x)\n",
    "    for _ in range(num_iter):\n",
    "        q_xz = p_x * q_z_x # Joint distribution q(x,z), shape X x Z\n",
    "        q_z = q_xz.sum(axis=0, keepdims=True) # Marginal distribution q(z), shape 1 x Z\n",
    "        q_y_z = ((q_xz / q_z)[:, :, None] * p_y_x).sum(axis=0, keepdims=True) # Conditional decoder distribution q(y|z), shape 1 x Z x Y\n",
    "        d = ( \n",
    "            scipy.special.xlogy(p_y_x, p_y_x)\n",
    "            - scipy.special.xlogy(p_y_x, q_y_z) # negative KL divergence -D[p(y|x) || q(y|z)]\n",
    "        ).sum(axis=-1) # expected distortion over Y; shape X x Z\n",
    "        q_z_x = scipy.special.softmax((np.log(q_z) - gamma*d)/temperature, axis=-1) # Conditional encoder distribution q(z|x) = 1/Z q(z) e^{-gamma*d}\n",
    "\n",
    "    return q_z_x\n",
    "\n",
    "num_dists = 3\n",
    "pgs_dists = [0,0.789,-1.315]\n",
    "num_words = 9\n",
    "num_meanings = num_dists * 3\n",
    "\n",
    "# function to make the non-deterministic frontier\n",
    "def make_curve(mu, logsp=np.logspace(2, 0, num=1500), pgs=pgs_dists):\n",
    "    init = np.identity(num_words)\n",
    "\n",
    "    qW_M = []\n",
    "    informativity = []\n",
    "    complexity = []\n",
    "\n",
    "    for gamma in logsp:\n",
    "        # x = RunIB(mu, gamma, num_dists, pgs_dists)\n",
    "        x = RunIB(mu, num_dists, pgs_dists)\n",
    "        p_m = x.prior\n",
    "        p_u_m = x.prob_u_given_m\n",
    "        p_u = p_u_m.sum(axis=-1, keepdims=True)\n",
    "        q_w_m = _ib(p_m, p_u_m, num_words, gamma, init, num_iter = 20)\n",
    "        informativity_temp, complexity_temp = information_plane(p_m, p_u, p_u_m, q_w_m)\n",
    "\n",
    "        qW_M.append(q_w_m)\n",
    "        informativity.append(informativity_temp)\n",
    "        complexity.append(complexity_temp)\n",
    "        init = q_w_m\n",
    "        \n",
    "    curve = pd.DataFrame(data = {'gamma': logsp,\n",
    "                    'informativity' : informativity,\n",
    "                    'complexity' : complexity,\n",
    "                    'J' : complexity - logsp*informativity})\n",
    "    return curve, qW_M\n",
    "\n",
    "# function to find the objective function\n",
    "def get_objective(p_m, p_u_m, q_w_m, gamma):\n",
    "    informativity, complexity = information_plane(p_m, p_u_m, q_w_m)\n",
    "    return complexity - gamma * informativity\n",
    "\n",
    "# function to find gamma minimizing the objective function\n",
    "def find_gamma_index(p_m, p_u_m, q_w_m, curve):\n",
    "    objs = np.array([get_objective(p_m, p_u_m, q_w_m, gamma) for gamma in logsp])\n",
    "    objs_2 = curve[\"J\"].values\n",
    "    diff = objs - objs_2\n",
    "    return(diff.argmin())\n",
    "\n",
    "# function to calculate efficiency loss (Zaslavsky et al., 2018)\n",
    "def find_epsilon(p_m, p_u_m, q_w_m, curve):\n",
    "    objs = np.array([get_objective(p_m, p_u_m, q_w_m, gamma) for gamma in logsp])\n",
    "    objs_2 = curve[\"J\"].values\n",
    "    diff = objs - objs_2\n",
    "    return(diff.min()/logsp[diff.argmin()])\n",
    "\n",
    "# codes from Zaslavsky et al. (2018)\n",
    "def xlogx(v):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        return np.where(v > PRECISION, v * np.log2(v), 0)\n",
    "    \n",
    "def H(p, axis=None):\n",
    "    \"\"\" Entropy \"\"\"\n",
    "    return -xlogx(p).sum(axis=axis)\n",
    "\n",
    "def MI(pXY):\n",
    "    \"\"\" mutual information, I(X;Y) \"\"\"\n",
    "    return H(pXY.sum(axis=0)) + H(pXY.sum(axis=1)) - H(pXY)\n",
    "\n",
    "# function to calculate gNID (Zaslavsky et al., 2018)\n",
    "def gNID(pW_X, pV_X, pX):\n",
    "    if len(pX.shape) == 1:\n",
    "        pX = pX[:, None]\n",
    "    elif pX.shape[0] == 1 and pX.shape[1] > 1:\n",
    "        pX = pX.T\n",
    "    pXW = pW_X * pX\n",
    "    pWV = pXW.T.dot(pV_X)\n",
    "    pWW = pXW.T.dot(pW_X)\n",
    "    pVV = (pV_X * pX).T.dot(pV_X)\n",
    "    score = 1 - MI(pWV) / (np.max([MI(pWW), MI(pVV)]))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e703afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "information_plane() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m logsp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogspace(\u001b[39m3\u001b[39m,\u001b[39m0\u001b[39m,num \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mu \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m curve, qW_M \u001b[39m=\u001b[39m make_curve(mu, logsp, pgs_dists)\n",
      "\u001b[1;32m/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb Cell 3\u001b[0m in \u001b[0;36mmake_curve\u001b[0;34m(mu, logsp, pgs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m complexity \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mfor\u001b[39;00m gamma \u001b[39min\u001b[39;00m logsp:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# x = RunIB(mu, gamma, num_dists, pgs_dists)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     x \u001b[39m=\u001b[39m RunIB(mu, num_dists, pgs_dists)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     p_m \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mprior\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cshnican/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/fit_gamma_new.ipynb#W2sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     p_u_m \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mprob_u_given_m\n",
      "File \u001b[0;32m~/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/run_ib_new.py:139\u001b[0m, in \u001b[0;36mRunIB.__init__\u001b[0;34m(self, mu, distal_levels, pgs_dists, prior_spec, prior_source, merge_d1d2)\u001b[0m\n\u001b[1;32m    137\u001b[0m p_um \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob_u_given_m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior[:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob_u \u001b[39m=\u001b[39m p_um\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m# p(u)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_lexicons, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_lexicon_informativity, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_lexicon_complexity, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimal_lexicon_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_ib_curve()\n",
      "File \u001b[0;32m~/Documents/BCS/TedLab/here_there_way_over_there/repo_published/spatial_demonstratives/run_ib_new.py:171\u001b[0m, in \u001b[0;36mRunIB.get_ib_curve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mfor\u001b[39;00m gamma \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogsp:\n\u001b[1;32m    170\u001b[0m     q_w_m \u001b[39m=\u001b[39m _ib(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprior, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob_u_given_m, num_words, gamma, init, num_iter\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m     informativity_temp, complexity_temp \u001b[39m=\u001b[39m information_plane(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprior, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprob_u, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprob_u_given_m, q_w_m)\n\u001b[1;32m    173\u001b[0m     qW_M\u001b[39m.\u001b[39mappend(q_w_m)\n\u001b[1;32m    174\u001b[0m     informativity\u001b[39m.\u001b[39mappend(informativity_temp)\n",
      "\u001b[0;31mTypeError\u001b[0m: information_plane() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# This part generates the non-deterministic optimal frontier, with gamma ranging from 1 to 1000\n",
    "logsp = np.logspace(3,0,num = 2000)\n",
    "mu = 0.2\n",
    "curve, qW_M = make_curve(mu, logsp, pgs_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part generates all possible lexicons\n",
    "x = RunIB(mu,2,num_dists, pgs_dists)\n",
    "num_meanings = 3 * num_dists\n",
    "lexicon_size_range = range(2, num_meanings + 1)\n",
    "sim_lex_dict = {lexicon_size: [lexicon for lexicon in enumerate_lexicons.enumerate_possible_lexicons(num_meanings, lexicon_size)] for \n",
    "        lexicon_size in lexicon_size_range}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36544621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame containing real lexicons\n",
    "lexicons = x.get_real_langs(num_meanings)\n",
    "\n",
    "df = pd.DataFrame([{dm: l[1].argmax(axis=1)[dm_num]\n",
    "                        for dm_num, dm in enumerate(x.deictic_index)} for l in lexicons])\n",
    "information_plane_list = [information_plane(x.prior, x.prob_u_given_m, l[1]) for l in lexicons]\n",
    "df[\"I[U;W]\"] = [l[0] for l in information_plane_list]\n",
    "df[\"I[M;W]\"] = [l[1] for l in information_plane_list]\n",
    "df[\"gamma_fit\"] = [logsp[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)] for l in lexicons]\n",
    "df[\"epsilon\"] = [find_epsilon(x.prior, x.prob_u_given_m, l[1], curve) for l in lexicons]\n",
    "df[\"gNID\"] = [gNID(l[1], qW_M[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)], x.prior) for l in lexicons]\n",
    "df[\"Language\"] = [l[0] for l in lexicons]\n",
    "df[\"Area\"] = [l[2] for l in lexicons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae786d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame containing simulated lexicons\n",
    "lexicons_sim = []\n",
    "for lexicon_size in range(2, num_meanings+1):\n",
    "    all_lex = sim_lex_dict[lexicon_size]\n",
    "    lexicons_sim += [(\"simulated\", l[1], \"simulated\") for l in all_lex]\n",
    "\n",
    "df_sim = pd.DataFrame([{dm: l[1].argmax(axis=1)[dm_num]\n",
    "                        for dm_num, dm in enumerate(x.deictic_index)} for l in lexicons_sim])\n",
    "\n",
    "information_plane_list_sim = [information_plane(x.prior, x.prob_u_given_m, l[1]) for l in lexicons_sim]\n",
    "df_sim[\"I[U;W]\"] = [l[0] for l in information_plane_list_sim]\n",
    "df_sim[\"I[M;W]\"] = [l[1] for l in information_plane_list_sim]\n",
    "df_sim[\"gamma_fit\"] = [logsp[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)] for l in lexicons_sim]\n",
    "df_sim[\"epsilon\"] = [find_epsilon(x.prior, x.prob_u_given_m, l[1], curve) for l in lexicons_sim]\n",
    "df_sim[\"gNID\"] = [gNID(l[1], qW_M[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)], x.prior) for l in lexicons_sim]\n",
    "df_sim[\"Language\"] = [l[0] for l in lexicons_sim]\n",
    "df_sim[\"Area\"] = [l[2] for l in lexicons_sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = systematicity(df)\n",
    "df_sim = systematicity(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f892ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save things\n",
    "df.to_csv('sheets/real_lexicons_fit_mu_' + str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) + '.csv')\n",
    "df_sim.to_csv('sheets/sim_lexicons_fit_mu_'+ str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) +'.csv')\n",
    "curve.to_csv('sheets/ib_curve_non_deter_mu_' + str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b98915",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.3\n",
    "num_dists = 3\n",
    "pgs_dists = [0,0.789,-1.315]\n",
    "num_words = 9\n",
    "num_meanings = num_dists * 3\n",
    "x = RunIB(mu,num_dists, pgs_dists)\n",
    "lexicons = x.get_real_langs(num_meanings)\n",
    "#temp = [run_ib_new.RunIB(mu, num_dists, pgs_dists).find_everything(l[1]) for l in lexicons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504acbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
